{"metadata": {"kernelspec": {"name": "mindspore-python3.7-aarch64", "display_name": "Mindspore-python3.7-aarch64", "language": "python"}, "language_info": {"name": "python", "version": "3.7.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "## 1.\u5bfc\u5165\u6a21\u5757", "metadata": {}}, {"cell_type": "code", "source": "import os\nimport numpy as np\nimport re\nimport sys\nimport random\nimport unicodedata\nimport math\n\nfrom mindspore import Tensor, nn, Model, context\nfrom mindspore.train.serialization import load_param_into_net, load_checkpoint\nfrom mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor\nfrom mindspore import dataset as ds\nfrom mindspore.mindrecord import FileWriter\nfrom mindspore import Parameter\nfrom mindspore.nn.loss.loss import _Loss\nfrom mindspore.ops import functional as F\nfrom mindspore.ops import operations as P\nfrom mindspore.common import dtype as mstype", "metadata": {"trusted": true}, "execution_count": 1, "outputs": [{"name": "stderr", "text": "[WARNING] ME(879:281472949586224,MainProcess):2021-03-29-00:56:58.698.70 [mindspore/_check_version.py:207] MindSpore version 1.1.1 and \"te\" wheel package version 1.0 does not match, reference to the match info on: https://www.mindspore.cn/install\nMindSpore version 1.1.1 and \"topi\" wheel package version 0.6.0 does not match, reference to the match info on: https://www.mindspore.cn/install\n[WARNING] ME(879:281472949586224,MainProcess):2021-03-29-00:56:58.864.357 [mindspore/ops/operations/array_ops.py:2302] WARN_DEPRECATED: The usage of Pack is deprecated. Please use Stack.\n", "output_type": "stream"}, {"name": "stdout", "text": "WARNING: 'ControlDepend' is deprecated from version 1.1 and will be removed in a future version, use 'Depend' instead.\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target='Ascend')", "metadata": {"trusted": true}, "execution_count": 2, "outputs": []}, {"cell_type": "markdown", "source": "## 2.\u6570\u636e\u9884\u5904\u7406", "metadata": {}}, {"cell_type": "markdown", "source": "### 2.1\u6570\u636e\u9884\u5904\u7406\u7ec6\u8282", "metadata": {}}, {"cell_type": "code", "source": "#\u67e5\u770b\u8bad\u7ec3\u6570\u636e\u5185\u5bb9\u524d10\u884c\u5185\u5bb9\nwith open(\"./data/cmn_zhsim.txt\", 'r', encoding='utf-8') as f:\n        for i in range(10):\n            print(f.readline())", "metadata": {"trusted": true}, "execution_count": 39, "outputs": [{"name": "stdout", "text": "Hi.\t\u55e8\u3002\n\nHi.\t\u4f60\u597d\u3002\n\nRun.\t\u4f60\u7528\u8dd1\u7684\u3002\n\nWait!\t\u7b49\u7b49\uff01\n\nHello!\t\u4f60\u597d\u3002\n\nI try.\t\u8ba9\u6211\u6765\u3002\n\nI won!\t\u6211\u8d62\u4e86\u3002\n\nOh no!\t\u4e0d\u4f1a\u5427\u3002\n\nCheers!\t\u5e72\u676f!\n\nHe ran.\t\u4ed6\u8dd1\u4e86\u3002\n\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "EOS = \"<eos>\"\nSOS = \"<sos>\"\nMAX_SEQ_LEN=10", "metadata": {"trusted": true}, "execution_count": 5, "outputs": []}, {"cell_type": "code", "source": "#\u6211\u4eec\u9700\u8981\u5c06\u5b57\u7b26\u8f6c\u5316\u4e3aASCII\u7f16\u7801\n#\u5e76\u5168\u90e8\u8f6c\u5316\u4e3a\u5c0f\u5199\u5b57\u6bcd\uff0c\u5e76\u4fee\u526a\u5927\u90e8\u5206\u6807\u70b9\u7b26\u53f7\n#\u9664\u4e86(a-z, A-Z, \".\", \"?\", \"!\", \",\")\u8fd9\u4e9b\u5b57\u7b26\u5916\uff0c\u5168\u66ff\u6362\u6210\u7a7a\u683c\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\ndef normalizeString(s):\n    s = s.lower().strip()\n    s = unicodeToAscii(s)\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s", "metadata": {"trusted": true}, "execution_count": 6, "outputs": []}, {"cell_type": "code", "source": "def prepare_data(data_path, vocab_save_path, max_seq_len):\n    with open(data_path, 'r', encoding='utf-8') as f:\n        data = f.read()\n\n    # \u8bfb\u53d6\u6587\u672c\u6587\u4ef6\uff0c\u6309\u884c\u5206\u5272\uff0c\u518d\u5c06\u6bcf\u884c\u5206\u5272\u6210\u8bed\u53e5\u5bf9\n    data = data.split('\\n')\n\n     # \u622a\u53d6\u524d2000\u884c\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\n    data = data[:2000]\n\n    # \u5206\u5272\u6bcf\u884c\u4e2d\u7684\u4e2d\u82f1\u6587\n    en_data = [normalizeString(line.split('\\t')[0]) for line in data]\n\n    ch_data = [line.split('\\t')[1] for line in data]\n\n    # \u5229\u7528\u96c6\u5408\uff0c\u83b7\u5f97\u4e2d\u82f1\u6587\u8bcd\u6c47\u8868\n    en_vocab = set(' '.join(en_data).split(' '))\n    id2en = [EOS] + [SOS] + list(en_vocab)\n    en2id = {c:i for i,c in enumerate(id2en)}\n    en_vocab_size = len(id2en)\n    # np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n\n    ch_vocab = set(''.join(ch_data))\n    id2ch = [EOS] + [SOS] + list(ch_vocab)\n    ch2id = {c:i for i,c in enumerate(id2ch)}\n    ch_vocab_size = len(id2ch)\n    # np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n\n    # \u5c06\u53e5\u5b50\u7528\u8bcd\u6c47\u8868id\u8868\u793a\n    en_num_data = np.array([[1] + [int(en2id[en]) for en in line.split(' ')] + [0] for line in en_data])\n    ch_num_data = np.array([[1] + [int(ch2id[ch]) for ch in line] + [0] for line in ch_data])\n\n    #\u5c06\u77ed\u53e5\u5b50\u6269\u5145\u5230\u7edf\u4e00\u7684\u957f\u5ea6\n    for i in range(len(en_num_data)):\n        num = max_seq_len + 1 - len(en_num_data[i])\n        if(num >= 0):\n            en_num_data[i] += [0]*num\n        else:\n            en_num_data[i] = en_num_data[i][:max_seq_len] + [0]\n\n    for i in range(len(ch_num_data)):\n        num = max_seq_len + 1 - len(ch_num_data[i])\n        if(num >= 0):\n            ch_num_data[i] += [0]*num\n        else:\n            ch_num_data[i] = ch_num_data[i][:max_seq_len] + [0]\n    \n    \n    np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n    \n    np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n\n    return en_num_data, ch_num_data, en_vocab_size, ch_vocab_size", "metadata": {"trusted": true}, "execution_count": 65, "outputs": []}, {"cell_type": "code", "source": "#\u5c06\u5904\u7406\u540e\u7684\u6570\u636e\u4fdd\u5b58\u4e3amindrecord\u6587\u4ef6\uff0c\u65b9\u4fbf\u540e\u7eed\u8bad\u7ec3\ndef convert_to_mindrecord(data_path, mindrecord_save_path, max_seq_len):\n    en_num_data, ch_num_data, en_vocab_size, ch_vocab_size = prepare_data(data_path, mindrecord_save_path, max_seq_len)\n\n    # \u8f93\u51fa\u524d\u5341\u884c\u82f1\u6587\u53e5\u5b50\u5bf9\u5e94\u7684\u6570\u636e\n    for i in range(10):\n        print(en_num_data[i])\n    \n    data_list_train = []\n    for en, ch in zip(en_num_data, ch_num_data):\n        en = np.array(en).astype(np.int32)\n        ch = np.array(ch).astype(np.int32)\n        data_json = {\"encoder_data\": en.reshape(-1),\n                     \"decoder_data\": ch.reshape(-1)}\n        data_list_train.append(data_json)\n    \n    data_list_eval = random.sample(data_list_train, 20)\n\n    data_dir = os.path.join(mindrecord_save_path, \"gru_train.mindrecord\")\n    writer = FileWriter(data_dir)\n    schema_json = {\"encoder_data\": {\"type\": \"int32\", \"shape\": [-1]},\n                   \"decoder_data\": {\"type\": \"int32\", \"shape\": [-1]}}\n    writer.add_schema(schema_json, \"gru_schema\")\n    writer.write_raw_data(data_list_train)\n    writer.commit()\n\n    data_dir = os.path.join(mindrecord_save_path, \"gru_eval.mindrecord\")\n    writer = FileWriter(data_dir)\n    writer.add_schema(schema_json, \"gru_schema\")\n    writer.write_raw_data(data_list_eval)\n    writer.commit()\n\n    print(\"en_vocab_size: \", en_vocab_size)\n    print(\"ch_vocab_size: \", ch_vocab_size)\n\n    return en_vocab_size, ch_vocab_size", "metadata": {"trusted": true}, "execution_count": 66, "outputs": []}, {"cell_type": "code", "source": "if not os.path.exists(\"./preprocess\"):\n    os.mkdir('./preprocess')\nconvert_to_mindrecord(\"./data/cmn_zhsim.txt\", './preprocess', MAX_SEQ_LEN)", "metadata": {"trusted": true}, "execution_count": 69, "outputs": [{"name": "stdout", "text": "[1, 624, 1061, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 624, 1061, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 288, 1061, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 819, 893, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 969, 893, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 427, 119, 1061, 0, 0, 0, 0, 0, 0, 0]\n[1, 427, 851, 893, 0, 0, 0, 0, 0, 0, 0]\n[1, 169, 101, 893, 0, 0, 0, 0, 0, 0, 0]\n[1, 378, 893, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 859, 446, 1061, 0, 0, 0, 0, 0, 0, 0]\nen_vocab_size:  1154\nch_vocab_size:  1116\n", "output_type": "stream"}, {"execution_count": 69, "output_type": "execute_result", "data": {"text/plain": "(1154, 1116)"}, "metadata": {}}]}, {"cell_type": "markdown", "source": "## 3.\u8bad\u7ec3\u8fc7\u7a0b", "metadata": {}}, {"cell_type": "markdown", "source": "### 3.1 \u8d85\u53c2\u6570\u8bbe\u7f6e", "metadata": {}}, {"cell_type": "code", "source": "from easydict import EasyDict as edict\n\n# CONFIG\ncfg = edict({\n    'en_vocab_size': 1154,\n    'ch_vocab_size': 1116,\n    'max_seq_length': 10,\n    'hidden_size': 1024,\n    'batch_size': 16,\n    'eval_batch_size': 1,\n    'learning_rate': 0.001,\n    'momentum': 0.9,\n    'num_epochs': 15,\n    'save_checkpoint_steps': 125,\n    'keep_checkpoint_max': 10,\n    'dataset_path':'./preprocess',\n    'ckpt_save_path':'./ckpt',\n    'checkpoint_path':'./ckpt/gru-15_125.ckpt'\n})", "metadata": {"trusted": true}, "execution_count": 21, "outputs": []}, {"cell_type": "markdown", "source": "### 3.2 \u8bfb\u53d6\u6570\u636e", "metadata": {}}, {"cell_type": "code", "source": "def target_operation(encoder_data, decoder_data):\n    encoder_data = encoder_data[1:]\n    target_data = decoder_data[1:]\n    decoder_data = decoder_data[:-1]\n    return encoder_data, decoder_data, target_data\n\ndef eval_operation(encoder_data, decoder_data):\n    encoder_data = encoder_data[1:]\n    decoder_data = decoder_data[:-1]\n    return encoder_data, decoder_data\n\ndef create_dataset(data_home, batch_size, repeat_num=1, is_training=True, device_num=1, rank=0):\n    if is_training:\n        data_dir = os.path.join(data_home, \"gru_train.mindrecord\")\n    else:\n        data_dir = os.path.join(data_home, \"gru_eval.mindrecord\")\n    data_set = ds.MindDataset(data_dir, columns_list=[\"encoder_data\",\"decoder_data\"], \n                              num_parallel_workers=4,\n                              num_shards=device_num, shard_id=rank)\n    if is_training:\n        operations = target_operation\n        data_set = data_set.map(operations=operations, \n                                input_columns=[\"encoder_data\",\"decoder_data\"],\n                    output_columns=[\"encoder_data\",\"decoder_data\",\"target_data\"],\n                    column_order=[\"encoder_data\",\"decoder_data\",\"target_data\"])\n    else:\n        operations = eval_operation\n        data_set = data_set.map(operations=operations, \n                                input_columns=[\"encoder_data\",\"decoder_data\"],\n                   output_columns=[\"encoder_data\",\"decoder_data\"],\n                   column_order=[\"encoder_data\",\"decoder_data\"])\n    data_set = data_set.shuffle(buffer_size=data_set.get_dataset_size())\n    data_set = data_set.batch(batch_size=batch_size, drop_remainder=True)\n    data_set = data_set.repeat(count=repeat_num)\n    return data_set", "metadata": {"trusted": true}, "execution_count": 71, "outputs": []}, {"cell_type": "code", "source": "ds_train = create_dataset(cfg.dataset_path, cfg.batch_size)", "metadata": {"trusted": true}, "execution_count": 72, "outputs": []}, {"cell_type": "markdown", "source": "### 3.3 \u7f51\u7edc\u6a21\u578b", "metadata": {}}, {"cell_type": "code", "source": "def gru_default_state(batch_size, input_size, hidden_size, num_layers=1, bidirectional=False):\n    '''Weight init for gru cell'''\n    stdv = 1 / math.sqrt(hidden_size)\n    weight_i = Parameter(Tensor(\n        np.random.uniform(-stdv, stdv, (input_size, 3*hidden_size)).astype(np.float32)), \n                         name='weight_i')\n    weight_h = Parameter(Tensor(\n        np.random.uniform(-stdv, stdv, (hidden_size, 3*hidden_size)).astype(np.float32)), \n                         name='weight_h')\n    bias_i = Parameter(Tensor(\n        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_i')\n    bias_h = Parameter(Tensor(\n        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_h')\n    return weight_i, weight_h, bias_i, bias_h\n\nclass GRU(nn.Cell):\n    def __init__(self, config, is_training=True):\n        super(GRU, self).__init__()\n        if is_training:\n            self.batch_size = config.batch_size\n        else:\n            self.batch_size = config.eval_batch_size\n        self.hidden_size = config.hidden_size\n        self.weight_i, self.weight_h, self.bias_i, self.bias_h = \\\n            gru_default_state(self.batch_size, self.hidden_size, self.hidden_size)\n        self.rnn = P.DynamicGRUV2()\n        self.cast = P.Cast()\n\n    def construct(self, x, hidden):\n        x = self.cast(x, mstype.float16)\n        y1, h1, _, _, _, _ = self.rnn(x, self.weight_i, self.weight_h, self.bias_i, self.bias_h, None, hidden)\n        return y1, h1\n\n\nclass Encoder(nn.Cell):\n    def __init__(self, config, is_training=True):\n        super(Encoder, self).__init__()\n        self.vocab_size = config.en_vocab_size\n        self.hidden_size = config.hidden_size\n        if is_training:\n            self.batch_size = config.batch_size\n        else:\n            self.batch_size = config.eval_batch_size\n\n        self.trans = P.Transpose()\n        self.perm = (1, 0, 2)\n        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n        self.h = Tensor(np.zeros((self.batch_size, self.hidden_size)).astype(np.float16))\n\n    def construct(self, encoder_input):\n        embeddings = self.embedding(encoder_input)\n        embeddings = self.trans(embeddings, self.perm)\n        output, hidden = self.gru(embeddings, self.h)\n        return output, hidden\n\nclass Decoder(nn.Cell):\n    def __init__(self, config, is_training=True, dropout=0.1):\n        super(Decoder, self).__init__()\n\n        self.vocab_size = config.ch_vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_len = config.max_seq_length\n\n        self.trans = P.Transpose()\n        self.perm = (1, 0, 2)\n        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n        self.dropout = nn.Dropout(1-dropout)\n        self.attn = nn.Dense(self.hidden_size, self.max_len)\n        self.softmax = nn.Softmax(axis=2)\n        self.bmm = P.BatchMatMul()\n        self.concat = P.Concat(axis=2)\n        self.attn_combine = nn.Dense(self.hidden_size * 2, self.hidden_size)\n\n        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n        self.out = nn.Dense(self.hidden_size, self.vocab_size)\n        self.logsoftmax = nn.LogSoftmax(axis=2)\n        self.cast = P.Cast()\n\n    def construct(self, decoder_input, hidden, encoder_output):\n        embeddings = self.embedding(decoder_input)\n        embeddings = self.dropout(embeddings)\n        # calculate attn\n        attn_weights = self.softmax(self.attn(embeddings))\n        encoder_output = self.trans(encoder_output, self.perm)\n        attn_applied = self.bmm(attn_weights, self.cast(encoder_output,mstype.float32))\n        output =  self.concat((embeddings, attn_applied))\n        output = self.attn_combine(output)\n\n\n        embeddings = self.trans(embeddings, self.perm)\n        output, hidden = self.gru(embeddings, hidden)\n        output = self.cast(output, mstype.float32)\n        output = self.out(output)\n        output = self.logsoftmax(output)\n\n        return output, hidden, attn_weights\n\nclass Seq2Seq(nn.Cell):\n    def __init__(self, config, is_train=True):\n        super(Seq2Seq, self).__init__()\n        self.max_len = config.max_seq_length\n        self.is_train = is_train\n\n        self.encoder = Encoder(config, is_train)\n        self.decoder = Decoder(config, is_train)\n        self.expanddims = P.ExpandDims()\n        self.squeeze = P.Squeeze(axis=0)\n        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n        self.concat = P.Concat(axis=1)\n        self.concat2 = P.Concat(axis=0)\n        self.select = P.Select()\n\n    def construct(self, src, dst):\n        encoder_output, hidden = self.encoder(src)\n        decoder_hidden = self.squeeze(encoder_output[self.max_len-2:self.max_len-1:1, ::, ::])\n        if self.is_train:\n            outputs, _ = self.decoder(dst, decoder_hidden, encoder_output)\n        else:\n            decoder_input = dst[::,0:1:1]\n            decoder_outputs = ()\n            for i in range(0, self.max_len):\n                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, \n                                                                 decoder_hidden, encoder_output)\n                decoder_hidden = self.squeeze(decoder_hidden)\n                decoder_output, _ = self.argmax(decoder_output)\n                decoder_output = self.squeeze(decoder_output)\n                decoder_outputs += (decoder_output,)\n                decoder_input = decoder_output\n            outputs = self.concat(decoder_outputs)\n        return outputs\n\nclass NLLLoss(_Loss):\n    '''\n       NLLLoss function\n    '''\n    def __init__(self, reduction='mean'):\n        super(NLLLoss, self).__init__(reduction)\n        self.one_hot = P.OneHot()\n        self.reduce_sum = P.ReduceSum()\n\n    def construct(self, logits, label):\n        label_one_hot = self.one_hot(label, F.shape(logits)[-1], F.scalar_to_array(1.0), \n                                     F.scalar_to_array(0.0))\n        #print('NLLLoss label_one_hot:',label_one_hot, label_one_hot.shape)\n        #print('NLLLoss logits:',logits, logits.shape)\n        #print('xxx:', logits * label_one_hot)\n        loss = self.reduce_sum(-1.0 * logits * label_one_hot, (1,))\n        return self.get_loss(loss)\n    \nclass WithLossCell(nn.Cell):\n    def __init__(self, backbone, config):\n        super(WithLossCell, self).__init__(auto_prefix=False)\n        self._backbone = backbone\n        self.batch_size = config.batch_size\n        self.onehot = nn.OneHot(depth=config.ch_vocab_size)\n        self._loss_fn = NLLLoss()\n        self.max_len = config.max_seq_length\n        self.squeeze = P.Squeeze()\n        self.cast = P.Cast()\n        self.argmax = P.ArgMaxWithValue(axis=1, keep_dims=True)\n        self.print = P.Print()\n\n    def construct(self, src, dst, label):\n        out = self._backbone(src, dst)\n        loss_total = 0\n        for i in range(self.batch_size):\n            loss = self._loss_fn(self.squeeze(out[::,i:i+1:1,::]), \n                                 self.squeeze(label[i:i+1:1, ::]))\n            loss_total += loss\n        loss = loss_total / self.batch_size\n        return loss", "metadata": {"trusted": true}, "execution_count": 24, "outputs": []}, {"cell_type": "code", "source": "network = Seq2Seq(cfg)\nnetwork = WithLossCell(network, cfg)\noptimizer = nn.Adam(network.trainable_params(), learning_rate=cfg.learning_rate, beta1=0.9, beta2=0.98)\nmodel = Model(network, optimizer=optimizer)", "metadata": {"trusted": true}, "execution_count": 25, "outputs": []}, {"cell_type": "markdown", "source": "### 3.4 \u6a21\u578b\u8bad\u7ec3", "metadata": {}}, {"cell_type": "code", "source": "loss_cb = LossMonitor()\nconfig_ck = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps, keep_checkpoint_max=cfg.keep_checkpoint_max)\nckpoint_cb = ModelCheckpoint(prefix=\"gru\", directory=cfg.ckpt_save_path, config=config_ck)\ntime_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\ncallbacks = [time_cb, ckpoint_cb, loss_cb]\n\nmodel.train(cfg.num_epochs, ds_train, callbacks=callbacks, dataset_sink_mode=True)", "metadata": {"trusted": true}, "execution_count": 26, "outputs": [{"name": "stdout", "text": "epoch: 1 step: 125, loss is 2.838659\nepoch time: 77082.398 ms, per step time: 616.659 ms\nepoch: 2 step: 125, loss is 2.5093772\nepoch time: 11390.392 ms, per step time: 91.123 ms\nepoch: 3 step: 125, loss is 1.8508363\nepoch time: 11374.811 ms, per step time: 90.998 ms\nepoch: 4 step: 125, loss is 1.8408301\nepoch time: 11262.442 ms, per step time: 90.100 ms\nepoch: 5 step: 125, loss is 1.3256171\nepoch time: 11227.342 ms, per step time: 89.819 ms\nepoch: 6 step: 125, loss is 1.0894792\nepoch time: 11234.876 ms, per step time: 89.879 ms\nepoch: 7 step: 125, loss is 0.80114794\nepoch time: 11338.926 ms, per step time: 90.711 ms\nepoch: 8 step: 125, loss is 0.7878199\nepoch time: 11508.560 ms, per step time: 92.068 ms\nepoch: 9 step: 125, loss is 0.35237563\nepoch time: 11505.715 ms, per step time: 92.046 ms\nepoch: 10 step: 125, loss is 0.31488234\nepoch time: 11493.019 ms, per step time: 91.944 ms\nepoch: 11 step: 125, loss is 0.19495572\nepoch time: 11536.087 ms, per step time: 92.289 ms\nepoch: 12 step: 125, loss is 0.11501935\nepoch time: 11556.475 ms, per step time: 92.452 ms\nepoch: 13 step: 125, loss is 0.13036935\nepoch time: 11534.854 ms, per step time: 92.279 ms\nepoch: 14 step: 125, loss is 0.20907374\nepoch time: 11541.235 ms, per step time: 92.330 ms\nepoch: 15 step: 125, loss is 0.069087505\nepoch time: 11546.282 ms, per step time: 92.370 ms\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "## 4.\u63a8\u7406\u90e8\u5206", "metadata": {}}, {"cell_type": "code", "source": "class InferCell(nn.Cell):\n    def __init__(self, network, config):\n        super(InferCell, self).__init__(auto_prefix=False)\n        self.expanddims = P.ExpandDims()\n        self.network = network\n\n    def construct(self, src, dst):\n        out = self.network(src, dst)\n        return out", "metadata": {"trusted": true}, "execution_count": 73, "outputs": []}, {"cell_type": "code", "source": "network = Seq2Seq(cfg,is_train=False)\nnetwork = InferCell(network, cfg)\nnetwork.set_train(False)\nparameter_dict = load_checkpoint(cfg.checkpoint_path)\nload_param_into_net(network, parameter_dict)\nmodel = Model(network)", "metadata": {"trusted": true}, "execution_count": 27, "outputs": []}, {"cell_type": "code", "source": "#\u6253\u5f00\u8bcd\u6c47\u8868\nwith open(os.path.join(cfg.dataset_path,\"en_vocab.txt\"), 'r', encoding='utf-8') as f:\n    data = f.read()\nen_vocab = list(data.split('\\n'))\n\nwith open(os.path.join(cfg.dataset_path,\"ch_vocab.txt\"), 'r', encoding='utf-8') as f:\n    data = f.read()\nch_vocab = list(data.split('\\n'))\n\ndef translate(str_en):\n    max_seq_len = 10\n    str_vocab = normalizeString(str_en).split(' ')\n    print(\"English\",str(str_vocab))\n    str_id = [[]]\n    for i in str_vocab:\n        str_id[0] += [en_vocab.index(i)]\n   \n    num = max_seq_len + 1 - len(str_id)\n    if(num >= 0):\n        str_id[0] += [0]*num\n    else:\n        str_id[0] = str_id[:max_seq_len] + [0]\n    str_id = Tensor(np.array(str_id).astype(np.int32))\n   \n    out_id = Tensor(np.array([[1,0]]).astype(np.int32))\n    \n    output = network(str_id, out_id)\n    out= ''\n    for x in output[0].asnumpy():\n        if x == 0:\n            break\n        out += ch_vocab[x]\n    print(\"\u4e2d\u6587\",out)", "metadata": {"trusted": true}, "execution_count": 28, "outputs": []}, {"cell_type": "code", "source": "translate('i love tom')", "metadata": {"trusted": true}, "execution_count": 75, "outputs": [{"name": "stdout", "text": "English ['i', 'love', 'tom']\n\u4e2d\u6587 \u6211\u7231\u6c64\u59c6\u3002\n", "output_type": "stream"}]}]}